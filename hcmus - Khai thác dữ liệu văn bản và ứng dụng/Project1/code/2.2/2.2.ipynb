{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNBZFLEMg+5b5W4PwNecHci"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"fnMSEeO6Pi9y","executionInfo":{"status":"ok","timestamp":1709902273946,"user_tz":-420,"elapsed":3395,"user":{"displayName":"Duy Phạm Trần Minh","userId":"06728994181393208734"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"34c1deb5-a6eb-4796-cbe7-9e4c367dfcd1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import AutoModel, AutoTokenizer\n","import numpy as np\n","import json"],"metadata":{"id":"0W5zsNKwwXzk","executionInfo":{"status":"ok","timestamp":1709902280108,"user_tz":-420,"elapsed":6164,"user":{"displayName":"Duy Phạm Trần Minh","userId":"06728994181393208734"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import os\n","# Get current working directory\n","current_directory = os.getcwd()\n","print(\"Current Directory:\", current_directory)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PGqP-YTGwnZi","executionInfo":{"status":"ok","timestamp":1709902280108,"user_tz":-420,"elapsed":8,"user":{"displayName":"Duy Phạm Trần Minh","userId":"06728994181393208734"}},"outputId":"9ddea9f6-00ae-49bf-cab8-567e21aab7a8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Current Directory: /content\n"]}]},{"cell_type":"code","source":["# List files and folders in the current directory\n","files_and_folders = os.listdir(current_directory)\n","print(\"Files and Folders:\")\n","for item in files_and_folders:\n","    print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2_ksCmC3w2Sb","executionInfo":{"status":"ok","timestamp":1709902280108,"user_tz":-420,"elapsed":6,"user":{"displayName":"Duy Phạm Trần Minh","userId":"06728994181393208734"}},"outputId":"252fba39-e412-4cb0-8d39-867961b10edc"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Files and Folders:\n",".config\n","Project1_Data.json\n","test.json\n","drive\n","2xT.json\n","train.json\n","P1.json\n","sample_data\n"]}]},{"cell_type":"code","source":["# Load PhoBERT model and tokenizer\n","phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n","tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n","\n","# Define your CNN-based classifier model\n","class Classifier(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(Classifier, self).__init__()\n","        # Define the CNN layers\n","        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=3)\n","        self.pool = nn.MaxPool1d(kernel_size=2)\n","        # Calculate the output size of CNN\n","        conv_output_size = self._get_conv_output_size(input_size)\n","        # Define the Fully Connected layer for classification\n","        self.fc = nn.Linear(conv_output_size, num_classes)\n","\n","    def forward(self, x):\n","        # Pass the input through CNN layers\n","        x = self.conv1(x)\n","        x = torch.relu(x)\n","        x = self.pool(x)\n","        # Flatten the output of CNN\n","        x = torch.flatten(x, 1)\n","        # Pass through Fully Connected layer\n","        x = self.fc(x)\n","        return x\n","\n","    def _get_conv_output_size(self, input_size):\n","        # Calculate the output size of CNN\n","        input_tensor = torch.randn(1, input_size, 256)  # Assuming input size of 256\n","        conv_output = self._forward_conv(input_tensor)\n","        conv_output_size = np.prod(conv_output.size())\n","        return conv_output_size\n","\n","    def _forward_conv(self, x):\n","        # Helper function to get the output size of CNN\n","        x = self.conv1(x)\n","        x = torch.relu(x)\n","        x = self.pool(x)\n","        return x\n","\n","# Function to preprocess data and extract embeddings using PhoBERT\n","def preprocess_data(data):\n","    embeddings = []\n","    labels = []\n","    for item in data:\n","        question = item[\"question\"]\n","        text = item[\"text\"]\n","        # Combine question and text into one string\n","        combined_text = f\"{question} {text}\"\n","        # Tokenize the combined text\n","        tokenized_text = tokenizer(combined_text, return_tensors=\"pt\", max_length=256, truncation=True, padding=\"max_length\")\n","        input_ids = tokenized_text[\"input_ids\"]\n","        # Extract embeddings using PhoBERT\n","        with torch.no_grad():\n","            features = phobert(input_ids)[0]\n","        embeddings.append(features.squeeze(0))\n","        # Add label\n","        labels.append(1 if item[\"label\"] else 0)\n","    return torch.stack(embeddings), torch.tensor(labels)\n","\n","# Load and preprocess data from test.json\n","with open(\"Project1_Data.json\", \"r\", encoding=\"utf-8\") as f:\n","    data = json.load(f)\n","\n","embeddings, labels = preprocess_data(data)\n","\n","# Define your CNN-based classifier model\n","input_size = embeddings.size(2)  # Dimension of the embeddings\n","hidden_size = 128  # Size of the hidden layer in CNN\n","num_classes = 2  # Number of output classes (answerable or not answerable)\n","model = Classifier(input_size, hidden_size, num_classes)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Define DataLoader for training\n","dataset = TensorDataset(embeddings, labels)\n","train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n","\n","# Training loop\n","epochs = 10\n","for epoch in range(epochs):\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data\n","        optimizer.zero_grad()\n","        outputs = model(inputs.permute(0, 2, 1))  # Permute the input to match CNN input shape\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        if i % 100 == 99:    # Print every 100 mini-batches\n","            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 100:.3f}\")\n","            running_loss = 0.0\n","\n","# Evaluate the model\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    total_correct = 0\n","    total_samples = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            outputs = model(inputs.permute(0, 2, 1))\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            total_correct += (predicted == labels).sum().item()\n","            total_samples += inputs.size(0)\n","\n","    avg_loss = total_loss / total_samples\n","    accuracy = total_correct / total_samples\n","\n","    return avg_loss, accuracy\n","\n","def split_data(embeddings, labels, split_ratio=0.8):\n","    total_samples = len(embeddings)\n","    split_index = int(total_samples * split_ratio)\n","\n","    train_embeddings = embeddings[:split_index]\n","    train_labels = labels[:split_index]\n","\n","    val_embeddings = embeddings[split_index:]\n","    val_labels = labels[split_index:]\n","\n","    return (train_embeddings, train_labels), (val_embeddings, val_labels)\n","\n","# Split data into train and validation sets\n","train_data, val_data = split_data(embeddings, labels)\n","\n","# Define DataLoader for validation\n","val_dataset = TensorDataset(val_data[0], val_data[1])\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","\n","# Evaluate the model on validation set\n","val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n","\n","print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2%}\")"],"metadata":{"id":"MFOWqsJHS7f5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709903210897,"user_tz":-420,"elapsed":9316,"user":{"displayName":"Duy Phạm Trần Minh","userId":"06728994181393208734"}},"outputId":"7ebbd9ca-e061-4d1c-944e-d64e873805f2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss: 0.0012, Validation Accuracy: 100.00%\n"]}]}]}